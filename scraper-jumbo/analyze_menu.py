#!/usr/bin/env python3
"""
An√°lisis del men√∫ de categor√≠as principal - Etapa 3.3
Filtrar y validar categor√≠as extra√≠das
"""

import sys
from pathlib import Path
import json
import requests
import time
import time
from urllib.parse import urljoin

# Agregar el directorio src al path
project_root = Path(__file__).parent
src_path = project_root / "src"
sys.path.insert(0, str(src_path))

from scraper import JumboScraper
from bs4 import BeautifulSoup
import re

def filter_and_validate_categories(input_file='categories_extracted.json', output_file='categories_filtered.json'):
    """Filtrar y validar categor√≠as - Etapa 3.3"""
    print('üöÄ INICIANDO FILTRADO Y VALIDACI√ìN DE CATEGOR√çAS - ETAPA 3.3')
    print('=' * 60)

    # Cargar categor√≠as extra√≠das
    try:
        with open(input_file, 'r', encoding='utf-8') as f:
            categories = json.load(f)
        print(f'üìÇ Cargadas {len(categories)} categor√≠as del archivo {input_file}')
    except FileNotFoundError:
        print(f'‚ùå Error: No se encontr√≥ el archivo {input_file}')
        return []
    except json.JSONDecodeError as e:
        print(f'‚ùå Error al leer JSON: {e}')
        return []

    # Categor√≠as a excluir (seg√∫n especificaci√≥n)
    categories_to_exclude = [
        'viv√≠ saludable', 'vivi saludable',  # Se considera no relevante
        'ofertas', 'novedades', 'promociones',  # Categor√≠as promocionales
        'servicios', 'atenci√≥n al cliente', 'contacto'  # No son categor√≠as de productos
    ]

    filtered_categories = []
    validated_categories = []

    print('\nüîç FILTRANDO CATEGOR√çAS NO RELEVANTES...')
    print('-' * 40)

    for category in categories:
        name_lower = category['name'].lower().strip()

        # Verificar si debe excluirse
        should_exclude = False
        for exclude_term in categories_to_exclude:
            if exclude_term in name_lower:
                should_exclude = True
                print(f'üö´ Excluyendo: "{category["name"]}" (contiene "{exclude_term}")')
                break

        if not should_exclude:
            filtered_categories.append(category)
            print(f'‚úÖ Manteniendo: "{category["name"]}"')

    print(f'\nüìä Despu√©s del filtrado: {len(filtered_categories)} categor√≠as')

    # Validar URLs
    print('\nüîó VALIDANDO URLs DE CATEGOR√çAS...')
    print('-' * 40)

    scraper = JumboScraper()
    base_url = 'https://www.jumbo.com.ar/'

    for i, category in enumerate(filtered_categories, 1):
        url = category['url']
        name = category['name']

        print(f'{i:2d}. Validando: {name}')
        print(f'    URL: {url}')

        try:
            # Hacer petici√≥n HEAD para validar URL (m√°s r√°pido que GET completo)
            response = requests.head(url, timeout=10, allow_redirects=True,
                                   headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'})

            if response.status_code == 200:
                print(f'    ‚úÖ URL v√°lida (Status: {response.status_code})')
                validated_categories.append(category)
            else:
                print(f'    ‚ö†Ô∏è  URL no v√°lida (Status: {response.status_code})')

        except requests.RequestException as e:
            print(f'    ‚ùå Error de conexi√≥n: {str(e)[:50]}...')

        # Peque√±a pausa para no sobrecargar el servidor
        time.sleep(0.5)

    print(f'\nüìä Despu√©s de validaci√≥n: {len(validated_categories)} categor√≠as v√°lidas')

    # Limpiar nombres de categor√≠as
    print('\nüßπ LIMPIANDO NOMBRES DE CATEGOR√çAS...')
    print('-' * 40)

    for category in validated_categories:
        original_name = category['name']
        # Limpiar nombre (remover espacios extra, caracteres especiales)
        clean_name = ' '.join(original_name.split())  # Normalizar espacios
        clean_name = clean_name.title()  # Capitalizar correctamente

        category['name'] = clean_name
        category['name_cleaned'] = clean_name != original_name

        if clean_name != original_name:
            print(f'üßΩ "{original_name}" ‚Üí "{clean_name}"')
        else:
            print(f'‚úÖ "{clean_name}" (sin cambios)')

    # Guardar categor√≠as filtradas y validadas
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(validated_categories, f, indent=2, ensure_ascii=False)

    print(f'\nüíæ CATEGOR√çAS GUARDADAS EN: {output_file}')

    # Mostrar resumen final
    print('\nüìä RESUMEN FINAL - ETAPA 3.3')
    print('=' * 40)
    print(f'üìÇ Categor√≠as iniciales: {len(categories)}')
    print(f'üîç Despu√©s del filtrado: {len(filtered_categories)}')
    print(f'üîó Despu√©s de validaci√≥n: {len(validated_categories)}')
    print(f'üíæ Archivo generado: {output_file}')

    if validated_categories:
        print('\nüìã LISTADO DE CATEGOR√çAS VALIDADAS:')
        print('-' * 40)
        for i, category in enumerate(validated_categories, 1):
            print(f'{i:2d}. {category["name"]}')

def extract_filters_from_category(scraper, category_url, category_name):
    """Extraer filtros de una categor√≠a espec√≠fica"""
    print(f'üîç Extrayendo filtros de: {category_name}')

    # Filtros base (siempre presentes seg√∫n especificaci√≥n)
    base_filters = ['Categor√≠a', 'Sub-Categor√≠a', 'Tipo de Producto']

    try:
        # Obtener HTML de la categor√≠a
        html_content = scraper.get_page(category_url)
        if not html_content:
            print(f'‚ùå Error obteniendo HTML para {category_name}')
            return base_filters

        soup = BeautifulSoup(html_content, 'html.parser')

        # Buscar elementos de filtro con diferentes estrategias
        filters = []

        # Estrategia 1: Buscar por clases relacionadas con filtros
        filter_selectors = [
            '.filter-item', '.facet-option', '.filter-option',
            '[data-filter]', '.search-filter', '.filter',
            '.facet', '.vtex-search-result-3-x-filterItem'
        ]

        for selector in filter_selectors:
            try:
                elements = soup.select(selector)
                for element in elements:
                    filter_text = element.get_text().strip()
                    if (filter_text and
                        len(filter_text) > 2 and
                        len(filter_text) < 100 and  # Evitar textos muy largos
                        filter_text not in base_filters and
                        'precio' not in filter_text.lower() and
                        'rango' not in filter_text.lower() and
                        not filter_text.isdigit()):  # Evitar n√∫meros sueltos

                        # Limpiar el texto
                        clean_text = re.sub(r'[^\w\s√°√©√≠√≥√∫√±√Å√â√ç√ì√ö√ë]', '', filter_text).strip()
                        if clean_text and len(clean_text) > 2:
                            filters.append(clean_text)

            except Exception as e:
                continue

        # Estrategia 2: Buscar en elementos con atributos data
        data_elements = soup.find_all(attrs={'data-filter': True})
        for element in data_elements:
            filter_text = element.get_text().strip()
            if (filter_text and
                len(filter_text) > 2 and
                filter_text not in base_filters and
                'precio' not in filter_text.lower()):
                clean_text = re.sub(r'[^\w\s√°√©√≠√≥√∫√±√Å√â√ç√ì√ö√ë]', '', filter_text).strip()
                if clean_text and len(clean_text) > 2:
                    filters.append(clean_text)

        # Estrategia 3: Buscar en listas de navegaci√≥n/filtros
        nav_elements = soup.find_all(['ul', 'ol'], class_=re.compile(r'filter|nav|facet'))
        for nav in nav_elements:
            list_items = nav.find_all('li')
            for item in list_items:
                filter_text = item.get_text().strip()
                if (filter_text and
                    len(filter_text) > 2 and
                    len(filter_text) < 50 and
                    filter_text not in base_filters and
                    'precio' not in filter_text.lower()):
                    clean_text = re.sub(r'[^\w\s√°√©√≠√≥√∫√±√Å√â√ç√ì√ö√ë]', '', filter_text).strip()
                    if clean_text and len(clean_text) > 2:
                        filters.append(clean_text)

        # Limpiar y deduplicar
        unique_filters = list(set(filters))
        unique_filters.sort()

        # Limitar a un m√°ximo razonable de filtros
        if len(unique_filters) > 50:
            unique_filters = unique_filters[:50]

        print(f'‚úÖ Encontrados {len(unique_filters)} filtros para {category_name}')

        # Retornar filtros base + filtros espec√≠ficos
        return base_filters + unique_filters

    except Exception as e:
        print(f'‚ùå Error extrayendo filtros de {category_name}: {e}')
        return base_filters  # Retornar al menos los filtros base

def extract_filters_from_all_categories(input_file='categories_filtered.json', output_file='categories_with_filters.json'):
    """Extraer filtros de todas las categor√≠as - Etapa 4"""
    print('üöÄ INICIANDO EXTRACCI√ìN DE FILTROS - ETAPA 4')
    print('=' * 50)

    # Cargar categor√≠as filtradas
    try:
        with open(input_file, 'r', encoding='utf-8') as f:
            categories = json.load(f)
        print(f'üìÇ Cargadas {len(categories)} categor√≠as del archivo {input_file}')
    except FileNotFoundError:
        print(f'‚ùå Error: No se encontr√≥ el archivo {input_file}')
        return []
    except json.JSONDecodeError as e:
        print(f'‚ùå Error al leer JSON: {e}')
        return []

    # Inicializar scraper
    scraper = JumboScraper()

    # Procesar cada categor√≠a
    processed_categories = []
    total_filters = 0

    print('\nüîç EXTRAYENDO FILTROS POR CATEGOR√çA...')
    print('-' * 40)

    for i, category in enumerate(categories, 1):
        print(f'\n{i:2d}/{len(categories)} Procesando: {category["name"]}')

        # Extraer filtros de la categor√≠a
        filters = extract_filters_from_category(scraper, category['url'], category['name'])

        # Agregar filtros a la categor√≠a
        category_with_filters = category.copy()
        category_with_filters['filters'] = filters
        category_with_filters['filters_count'] = len(filters)

        processed_categories.append(category_with_filters)
        total_filters += len(filters)

        print(f'   üìä Filtros extra√≠dos: {len(filters)}')

        # Peque√±a pausa para no sobrecargar el servidor
        time.sleep(1)

    # Guardar resultados
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(processed_categories, f, indent=2, ensure_ascii=False)

    print(f'\nüíæ RESULTADOS GUARDADOS EN: {output_file}')

    # Mostrar resumen
    print('\nüìä RESUMEN DE EXTRACCI√ìN - ETAPA 4')
    print('=' * 40)
    print(f'üìÇ Categor√≠as procesadas: {len(processed_categories)}')
    print(f'üîç Total de filtros extra√≠dos: {total_filters}')
    print(f'üìä Promedio de filtros por categor√≠a: {total_filters/len(processed_categories):.1f}')
    print(f'üíæ Archivo generado: {output_file}')

    # Mostrar top categor√≠as por cantidad de filtros
    print('\nüèÜ TOP 5 CATEGOR√çAS CON M√ÅS FILTROS:')
    print('-' * 40)

    sorted_categories = sorted(processed_categories, key=lambda x: x['filters_count'], reverse=True)
    for i, category in enumerate(sorted_categories[:5], 1):
        print(f'{i}. {category["name"]}: {category["filters_count"]} filtros')

    return processed_categories

def extract_categories_from_menu(driver):
    """Extraer todas las categor√≠as del men√∫ desplegado"""
    categories = []

    try:
        # Esperar a que el men√∫ se despliegue completamente
        time.sleep(3)

        # Buscar espec√≠ficamente dentro del men√∫ desplegado VTEX
        # Primero intentar encontrar el contenedor del men√∫ desplegado
        menu_container_selectors = [
            '.vtex-menu-2-x-menuContainer',  # Contenedor principal del men√∫
            '.vtex-menu-2-x-submenu',       # Submen√∫ desplegado
            '[class*="menuContainer"]',     # Contenedor gen√©rico
            '.vtex-menu-2-x-menuItem',      # Items del men√∫
        ]

        menu_container = None
        for selector in menu_container_selectors:
            try:
                containers = driver.find_elements(By.CSS_SELECTOR, selector)
                for container in containers:
                    # Verificar si el contenedor es visible y tiene contenido
                    if container.is_displayed() and container.size['height'] > 50:
                        menu_container = container
                        print(f'‚úÖ Contenedor del men√∫ encontrado: {selector}')
                        break
                if menu_container:
                    break
            except:
                continue

        if not menu_container:
            print('‚ö†Ô∏è  No se encontr√≥ contenedor espec√≠fico del men√∫, usando p√°gina completa')

        # Selectores m√°s espec√≠ficos para categor√≠as del men√∫ desplegado
        category_selectors = [
            '.vtex-menu-2-x-menuItem a[href*="/"]',           # Enlaces en items del men√∫ VTEX
            '.vtex-menu-2-x-submenuItem a[href*="/"]',       # Items del submen√∫
            '.vtex-menu-2-x-styledLink[href*="/"]',          # Enlaces estilizados del men√∫
        ]

        found_links = set()  # Evitar duplicados

        for selector in category_selectors:
            try:
                if menu_container:
                    # Buscar dentro del contenedor del men√∫
                    elements = menu_container.find_elements(By.CSS_SELECTOR, selector.replace('.vtex-menu-2-x-menuItem ', ''))
                else:
                    # Buscar en toda la p√°gina
                    elements = driver.find_elements(By.CSS_SELECTOR, selector)

                print(f'üîç Selector {selector}: {len(elements)} elementos encontrados')

                for element in elements:
                    try:
                        href = element.get_attribute('href')
                        text = element.text.strip()

                        if href and text and len(text) > 1 and len(text) < 50:
                            # Filtrar URLs v√°lidas de categor√≠as principales
                            if (href.startswith('https://www.jumbo.com.ar/') and
                                not href.endswith('/p') and  # No productos individuales
                                not 'javascript:' in href and
                                not '#' in href and
                                not '/p/' in href and       # No p√°ginas de producto
                                not 'descuentos' in href.lower() and  # No descuentos
                                not 'sucursales' in href.lower() and  # No sucursales
                                not 'arrepentimiento' in href.lower() and  # No bot√≥n arrepentimiento
                                not 'legales' in text.lower() and     # No legales
                                not 'bancarios' in text.lower()):     # No bancarios

                                # Limpiar el texto (remover caracteres extra√±os)
                                clean_text = re.sub(r'[^\w\s√°√©√≠√≥√∫√±√Å√â√ç√ì√ö√ë]', '', text).strip()

                                if clean_text and len(clean_text) > 2:
                                    # Verificar que no sea un enlace gen√©rico
                                    generic_terms = [
                                        'ver m√°s', 'ver todo', 'todos', 'ofertas', 'novedades',
                                        'comprar', 'compra', 'inicio', 'home', 'contacto',
                                        'ayuda', 'servicio', 'atenci√≥n', 'sucursales',
                                        'legales', 'bancarios', 'arrepentimiento'
                                    ]

                                    is_generic = False
                                    for term in generic_terms:
                                        if term in clean_text.lower():
                                            is_generic = True
                                            break

                                    if not is_generic:
                                        category_entry = {
                                            'name': clean_text,
                                            'url': href,
                                            'text_original': text
                                        }

                                        # Usar URL como clave para evitar duplicados
                                        if href not in found_links:
                                            found_links.add(href)
                                            categories.append(category_entry)
                                            print(f'‚úÖ Categor√≠a encontrada: {clean_text}')

                    except Exception as e:
                        print(f'‚ö†Ô∏è  Error procesando elemento: {e}')
                        continue

            except Exception as e:
                print(f'‚ö†Ô∏è  Selector {selector} fall√≥: {e}')
                continue

        # Filtrar categor√≠as principales (√∫ltima validaci√≥n)
        main_categories = []
        exclude_keywords = [
            'ver m√°s', 'ver todo', 'todos', 'ofertas', 'novedades',
            'comprar', 'compra', 'inicio', 'home', 'contacto',
            'ayuda', 'servicio', 'atenci√≥n', 'sucursales', 'legales',
            'bancarios', 'arrepentimiento', 'descuentos'
        ]

        for category in categories:
            name_lower = category['name'].lower()
            should_exclude = False

            for keyword in exclude_keywords:
                if keyword in name_lower:
                    should_exclude = True
                    break

            if not should_exclude and len(category['name']) > 2:
                main_categories.append(category)

        print(f'üìä Categor√≠as v√°lidas despu√©s del filtrado: {len(main_categories)}')
        return main_categories

    except Exception as e:
        print(f'‚ùå Error extrayendo categor√≠as: {e}')
        return []

def analyze_main_menu():
    """Extraer categor√≠as principales del men√∫ desplegado - Etapa 3.2"""
    scraper = JumboScraper()
    url = 'https://www.jumbo.com.ar/'

    print('üöÄ INICIANDO EXTRACCI√ìN DE CATEGOR√çAS - ETAPA 3.2')
    print('=' * 50)

    # Configurar Selenium con Chrome
    chrome_options = Options()
    chrome_options.add_argument("--start-maximized")  # Abrir en pantalla completa
    chrome_options.add_argument("--disable-web-security")
    chrome_options.add_argument("--disable-features=VizDisplayCompositor")

    try:
        driver = webdriver.Chrome(options=chrome_options)
        print('‚úÖ NAVEGADOR SELENIUM INICIADO')
    except Exception as e:
        print(f'‚ùå Error iniciando Selenium: {e}')
        print('Aseg√∫rate de tener ChromeDriver instalado')
        return

    try:
        print('üì° CARGANDO P√ÅGINA EN SELENIUM...')
        driver.get(url)
        time.sleep(3)  # Esperar a que cargue la p√°gina

        print('üîç BUSCANDO ELEMENTO DEL MEN√ö DESPLEGABLE...')

        # Buscar el elemento span con las clases espec√≠ficas
        try:
            menu_trigger = driver.find_element(By.CSS_SELECTOR, 'span.vtex-menu-2-x-styledLink--header-category')
            print('‚úÖ ELEMENTO TRIGGER DEL MEN√ö ENCONTRADO')
            print(f'Texto del elemento: "{menu_trigger.text}"')

            if menu_trigger.text.upper() == 'CATEGOR√çAS':
                print('üéØ ELEMENTO CORRECTO IDENTIFICADO: "CATEGOR√çAS"')
            else:
                print(f'‚ö†Ô∏è  Texto encontrado: "{menu_trigger.text}" (esperaba "CATEGOR√çAS")')

        except Exception as e:
            print(f'‚ùå Error encontrando el elemento: {e}')
            print('Buscando alternativa con XPath...')

            try:
                # Buscar por XPath como alternativa
                menu_trigger = driver.find_element(By.XPATH, "//span[contains(@class, 'vtex-menu-2-x-styledLink--header-category')]")
                print('‚úÖ ELEMENTO ENCONTRADO CON XPATH')
                print(f'Texto del elemento: "{menu_trigger.text}"')
            except Exception as e2:
                print(f'‚ùå Error con XPath tambi√©n: {e2}')
                return

        print('\nüñ±Ô∏è  HACIENDO HOVER SOBRE EL ELEMENTO...')
        print('=' * 50)

        # Crear ActionChains para hacer hover
        actions = ActionChains(driver)

        # Mover el mouse al elemento (hover)
        actions.move_to_element(menu_trigger).perform()

        print('‚úÖ HOVER REALIZADO')
        print('üìã MEN√ö DE CATEGOR√çAS DESPLEGADO')

        # Extraer categor√≠as del men√∫ desplegado
        print('\nüîç EXTRAYENDO CATEGOR√çAS DEL MEN√ö DESPLEGADO...')
        print('=' * 50)

        categories = extract_categories_from_menu(driver)

        print(f'üìä CATEGOR√çAS EXTRA√çDAS: {len(categories)}')

        if categories:
            print('\nüìã LISTADO DE CATEGOR√çAS ENCONTRADAS:')
            print('=' * 50)

            for i, category in enumerate(categories, 1):
                print(f'{i:2d}. {category["name"]}')
                print(f'    URL: {category["url"]}')

            # Guardar categor√≠as en archivo JSON
            output_file = 'categories_extracted.json'
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(categories, f, indent=2, ensure_ascii=False)

            print(f'\nüíæ CATEGOR√çAS GUARDADAS EN: {output_file}')

            # Mostrar resumen
            print(f'\nüìä RESUMEN DE EXTRACCI√ìN:')
            print(f'   - Total de categor√≠as: {len(categories)}')
            print(f'   - Archivo generado: {output_file}')

        else:
            print('‚ùå No se encontraron categor√≠as en el men√∫ desplegado')

        # Mantener la p√°gina abierta para verificaci√≥n
        print('\n‚è≥ MANTENIENDO P√ÅGINA ABIERTA PARA VERIFICACI√ìN...')
        print('üëÄ Verifica las categor√≠as extra√≠das y el men√∫ desplegado')
        print('Presiona Ctrl+C cuando hayas terminado de verificar')

        try:
            while True:
                time.sleep(1)
        except KeyboardInterrupt:
            print('\n‚úÖ VERIFICACI√ìN COMPLETADA POR EL USUARIO')

    except Exception as e:
        print(f'‚ùå Error durante la ejecuci√≥n: {e}')
    finally:
        print('\nüîÑ CERRANDO NAVEGADOR...')
        try:
            driver.quit()
        except:
            pass

    print('\n‚úÖ TAREA 3.2 COMPLETADA')
    print('üñ±Ô∏è  HOVER REALIZADO Y CATEGOR√çAS EXTRA√çDAS')
    print(f'ÔøΩ TOTAL DE CATEGOR√çAS: {len(categories) if "categories" in locals() else 0}')

def generate_markdown_report(input_file='categories_with_filters.json', output_file='categorias_jumbo.md'):
    """Generar archivo Markdown con categor√≠as y filtros - Etapa 5"""
    print('üöÄ GENERANDO REPORTE MARKDOWN - ETAPA 5')
    print('=' * 50)

    # Cargar categor√≠as con filtros
    try:
        with open(input_file, 'r', encoding='utf-8') as f:
            categories = json.load(f)
        print(f'üìÇ Cargadas {len(categories)} categor√≠as con filtros')
    except FileNotFoundError:
        print(f'‚ùå Error: No se encontr√≥ el archivo {input_file}')
        return
    except json.JSONDecodeError as e:
        print(f'‚ùå Error al leer JSON: {e}')
        return

    # Generar archivo Markdown
    with open(output_file, 'w', encoding='utf-8') as f:
        # Encabezado
        f.write('# Categor√≠as y Filtros - Jumbo Argentina\n\n')
        f.write('**Generado autom√°ticamente**\n\n')
        f.write(f'**Fecha:** {time.strftime("%Y-%m-%d %H:%M:%S")}\n\n')
        f.write(f'**Total de categor√≠as:** {len(categories)}\n\n')

        # Secci√≥n de categor√≠as
        f.write('## Categor√≠as\n\n')
        for i, category in enumerate(categories, 1):
            f.write(f'{i}. **{category["name"]}**: {category["url"]}\n')
        f.write('\n')

        # Secci√≥n de filtros por categor√≠a
        f.write('## Filtros por Categor√≠a\n\n')

        for category in categories:
            f.write(f'### {category["name"]}\n')
            filters = category.get('filters', [])
            total_filters = len(filters)

            f.write(f'**Total de filtros: {total_filters}**\n')

            if filters:
                # Filtros base (siempre presentes)
                f.write('-- FiltrosCategory\n')
                if 'Categor√≠a' in filters:
                    f.write('Categor√≠a\n')
                if 'Sub-Categor√≠a' in filters:
                    f.write('Sub-Categor√≠a\n')

                f.write('-- Tipo de producto\n')
                if 'Tipo de Producto' in filters:
                    f.write('Tipo de Producto\n')

                # Filtros espec√≠ficos
                f.write('-- Subfiltros\n')
                for filter_name in filters:
                    if filter_name not in ['Categor√≠a', 'Sub-Categor√≠a', 'Tipo de Producto']:
                        f.write(f'{filter_name}\n')

            f.write('\n')

    print(f'‚úÖ Archivo Markdown generado: {output_file}')
    print(f'üìä Total de categor√≠as procesadas: {len(categories)}')

    # Mostrar resumen
    total_filters_all = sum(len(cat.get('filters', [])) for cat in categories)
    print(f'üìã Total de filtros extra√≠dos: {total_filters_all}')

    return output_file

if __name__ == "__main__":
    generate_markdown_report()
