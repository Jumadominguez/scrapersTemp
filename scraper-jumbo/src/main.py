#!/usr/bin/env python3
"""
Scraper Jumbo - Extracci√≥n autom√°tica de filtros
Punto de entrada principal del script

Uso:
    python main.py
    python main.py --config /path/to/config.yaml
    python main.py --verbose
    python main.py --test-connection
    python main.py --site-info
    python main.py --validate-content
"""

import sys
import argparse
import logging
from pathlib import Path

# Agregar el directorio src al path
project_root = Path(__file__).parent.parent
src_path = project_root / "src"
sys.path.insert(0, str(src_path))

from config import initialize_config, get_logger
from scraper import JumboScraper
from extractor import extract_categories, extract_filters_from_category
from generator import generate_markdown


def parse_arguments():
    """Parsea los argumentos de l√≠nea de comandos"""
    parser = argparse.ArgumentParser(description='Scraper Jumbo - Extracci√≥n de filtros')

    parser.add_argument(
        '--config',
        type=str,
        help='Ruta al archivo de configuraci√≥n YAML'
    )

    parser.add_argument(
        '--verbose', '-v',
        action='store_true',
        help='Modo verbose con m√°s informaci√≥n de debug'
    )

    parser.add_argument(
        '--test-connection',
        action='store_true',
        help='Solo probar conexi√≥n al sitio web'
    )

    parser.add_argument(
        '--site-info',
        action='store_true',
        help='Mostrar informaci√≥n del sitio web'
    )

    parser.add_argument(
        '--validate-content',
        action='store_true',
        help='Validar contenido del sitio web'
    )

    return parser.parse_args()


def test_connection(scraper, logger):
    """Prueba la conexi√≥n b√°sica al sitio web"""
    logger.info("Probando conexi√≥n a Jumbo...")

    try:
        content = scraper.get_page("https://www.jumbo.com.ar")
        if content and "Jumbo" in content:
            logger.info("‚úÖ Conexi√≥n exitosa a Jumbo")
            return True
        else:
            logger.error("‚ùå Contenido no v√°lido recibido")
            return False
    except Exception as e:
        logger.error(f"‚ùå Error de conexi√≥n: {e}")
        return False


def main():
    """Funci√≥n principal del scraper"""
    args = parse_arguments()

    # Inicializar configuraci√≥n y logging
    try:
        config, logger = initialize_config()
        logger.info("üöÄ Iniciando Scraper Jumbo v1.0")
        logger.info(f"üìÅ Directorio de trabajo: {project_root}")

    except Exception as e:
        print(f"‚ùå Error al inicializar configuraci√≥n: {e}")
        sys.exit(1)

    # Override log level si se especifica verbose
    if args.verbose:
        logger.setLevel(logging.DEBUG)
        logger.info("üîç Modo verbose activado")

    # Crear instancia del scraper
    try:
        scraper = JumboScraper()
        logger.info("üîß Scraper inicializado correctamente")
    except Exception as e:
        logger.error(f"‚ùå Error al inicializar scraper: {e}")
        sys.exit(1)

    # Manejar opciones espec√≠ficas de la Etapa 2
    if args.test_connection:
        success = scraper.test_connection()
        sys.exit(0 if success else 1)

    if args.site_info:
        site_info = scraper.get_site_info()
        logger.info("üìä Informaci√≥n del sitio:")
        for key, value in site_info.items():
            logger.info(f"  {key}: {value}")
        sys.exit(0)

    if args.validate_content:
        content = scraper.get_page(config['site_url'])
        if content:
            is_valid = scraper._validate_jumbo_content(content)
            logger.info(f"‚úÖ Contenido v√°lido: {is_valid}")
            sys.exit(0 if is_valid else 1)
        else:
            logger.error("‚ùå No se pudo obtener contenido para validar")
            sys.exit(1)

    # Flujo principal de extracci√≥n
    try:
        logger.info("üåê Obteniendo p√°gina principal...")

        # 1. Obtener p√°gina principal
        main_page_content = scraper.get_page(config['site_url'])
        if not main_page_content:
            logger.error("‚ùå No se pudo obtener la p√°gina principal")
            sys.exit(1)

        # 2. Extraer categor√≠as
        logger.info("üìÇ Extrayendo categor√≠as...")
        categories = extract_categories(main_page_content)

        if not categories:
            logger.warning("‚ö†Ô∏è No se encontraron categor√≠as")
            sys.exit(1)

        logger.info(f"üìã Encontradas {len(categories)} categor√≠as")

        # 3. Procesar cada categor√≠a
        for i, category in enumerate(categories, 1):
            logger.info(f"üîç Procesando categor√≠a {i}/{len(categories)}: {category['name']}")

            # Extraer filtros de la categor√≠a
            filters = extract_filters_from_category(scraper, category['url'])
            category['filters'] = filters

            logger.info(f"‚úÖ Extra√≠dos {len(filters)} filtros para {category['name']}")

        # 4. Generar archivo Markdown
        logger.info("üìù Generando archivo Markdown...")
        output_path = project_root / config['output_file']
        generate_markdown(categories, str(output_path))

        logger.info(f"‚úÖ Archivo generado: {output_path}")
        logger.info("üéâ ¬°Extracci√≥n completada exitosamente!")

    except KeyboardInterrupt:
        logger.info("‚èπÔ∏è Proceso interrumpido por el usuario")
        sys.exit(1)
    except Exception as e:
        logger.error(f"‚ùå Error durante la extracci√≥n: {e}")
        logger.debug("Traceback completo:", exc_info=True)
        sys.exit(1)


if __name__ == "__main__":
    main()
